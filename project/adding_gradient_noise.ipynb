{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn.metrics\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# functions/classes taken from: https://gist.github.com/t-vi/9f6118ff84867e89f3348707c7a1271f\n",
    "# to help create validation set\n",
    "\n",
    "class PartialDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, parent_ds, offset, length):\n",
    "        self.parent_ds = parent_ds\n",
    "        self.offset = offset\n",
    "        self.length = length\n",
    "        assert len(parent_ds)>=offset+length, Exception(\"Parent Dataset not long enough\")\n",
    "        super(PartialDataset, self).__init__()\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    def __getitem__(self, i):\n",
    "        return self.parent_ds[i+self.offset]\n",
    "\n",
    "def validation_split(dataset, val_share=0.1):\n",
    "    \"\"\"\n",
    "       Split a (training and vaidation combined) dataset into training and validation.\n",
    "       Note that to be statistically sound, the items in the dataset should be statistically\n",
    "       independent (e.g. not sorted by class, not several instances of the same dataset that\n",
    "       could end up in either set).\n",
    "    \n",
    "       inputs:\n",
    "          dataset:   (\"training\") dataset to split into training and validation\n",
    "          val_share: fraction of validation data (should be 0<val_share<1, default: 0.1)\n",
    "       returns: input dataset split into test_ds, val_ds\n",
    "       \n",
    "       \"\"\"\n",
    "    val_offset = int(len(dataset)*(1-val_share))\n",
    "    return PartialDataset(dataset, 0, val_offset), PartialDataset(dataset, val_offset, len(dataset)-val_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "# load in MNIST\n",
    "train_validation = torchvision.datasets.MNIST(root = os.getcwd(), train=True, transform=torchvision.transforms.ToTensor(), download = True)\n",
    "train, validation = validation_split(train_validation, 1/6) # get train/validation split\n",
    "test = torchvision.datasets.MNIST(root = os.getcwd(), train=False, transform=torchvision.transforms.ToTensor(), download = True)\n",
    "\n",
    "# create train, validation, and test batches\n",
    "batch_size = 256\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train, batch_size = batch_size, shuffle = True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset = validation, batch_size = batch_size, shuffle = False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deep NN architecture\n",
    "# 12 layers, ReLU activations functions\n",
    "class Deep_FFNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Deep_FFNN, self).__init__()\n",
    "        \n",
    "        self.linear1 = torch.nn.Linear(in_features = input_size, out_features = hidden_size, bias = True).cuda()\n",
    "        self.relu1 = torch.nn.ReLU().cuda()\n",
    "        self.linear2 = torch.nn.Linear(in_features = hidden_size, out_features = hidden_size, bias = True).cuda()\n",
    "        self.relu2 = torch.nn.ReLU().cuda()\n",
    "        self.linear3 = torch.nn.Linear(in_features = hidden_size, out_features = hidden_size, bias = True).cuda()\n",
    "        self.relu3 = torch.nn.ReLU().cuda()\n",
    "        self.linear4 = torch.nn.Linear(in_features = hidden_size, out_features = hidden_size, bias = True).cuda()\n",
    "        self.relu4 = torch.nn.ReLU().cuda()\n",
    "        self.linear5 = torch.nn.Linear(in_features = hidden_size, out_features = hidden_size, bias = True).cuda()\n",
    "        self.relu5 = torch.nn.ReLU().cuda()\n",
    "        self.linear6 = torch.nn.Linear(in_features = hidden_size, out_features = hidden_size, bias = True).cuda()\n",
    "        self.relu6 = torch.nn.ReLU().cuda()\n",
    "        self.linear7 = torch.nn.Linear(in_features = hidden_size, out_features = hidden_size, bias = True).cuda()\n",
    "        self.relu7 = torch.nn.ReLU().cuda()\n",
    "        self.linear8 = torch.nn.Linear(in_features = hidden_size, out_features = hidden_size, bias = True).cuda()\n",
    "        self.relu8 = torch.nn.ReLU().cuda()\n",
    "        self.linear9 = torch.nn.Linear(in_features = hidden_size, out_features = hidden_size, bias = True).cuda()\n",
    "        self.relu9 = torch.nn.ReLU().cuda()\n",
    "        self.linear10 = torch.nn.Linear(in_features = hidden_size, out_features = hidden_size, bias = True).cuda()\n",
    "        self.relu10 = torch.nn.ReLU().cuda()\n",
    "        self.linear11 = torch.nn.Linear(in_features = hidden_size, out_features = hidden_size, bias = True).cuda()\n",
    "        self.relu11 = torch.nn.ReLU().cuda()\n",
    "        self.linear12 = torch.nn.Linear(in_features = hidden_size, out_features = hidden_size, bias = True).cuda()\n",
    "        self.relu12 = torch.nn.ReLU().cuda()\n",
    "        self.linearOut = torch.nn.Linear(in_features = hidden_size, out_features = 10, bias = True).cuda()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        hidden = self.linear1(input)\n",
    "        hidden = self.relu1(hidden)\n",
    "        hidden = self.linear2(hidden)\n",
    "        hidden = self.relu2(hidden)\n",
    "        hidden = self.linear3(hidden)\n",
    "        hidden = self.relu3(hidden)\n",
    "        hidden = self.linear4(hidden)\n",
    "        hidden = self.relu4(hidden)\n",
    "        hidden = self.linear5(hidden)\n",
    "        hidden = self.relu5(hidden)\n",
    "        hidden = self.linear6(hidden)\n",
    "        hidden = self.relu6(hidden)\n",
    "        hidden = self.linear7(hidden)\n",
    "        hidden = self.relu7(hidden)\n",
    "        hidden = self.linear8(hidden)\n",
    "        hidden = self.relu8(hidden)\n",
    "        hidden = self.linear9(hidden)\n",
    "        hidden = self.relu9(hidden)\n",
    "        hidden = self.linear10(hidden)\n",
    "        hidden = self.relu10(hidden)\n",
    "        hidden = self.linear11(hidden)\n",
    "        hidden = self.relu11(hidden)\n",
    "        hidden = self.linear12(hidden)\n",
    "        hidden = self.relu12(hidden)\n",
    "        output = self.linearOut(hidden)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for computing accuracy\n",
    "def compute_accuracy(data_loader, model, input_size):\n",
    "    model_pred = []\n",
    "    targets = []\n",
    "    for batch in data_loader:\n",
    "        model_output = model(Variable(torch.squeeze(batch[0], 1).view(len(batch[0]), input_size)).cuda()) # output from NN\n",
    "        model_probs = softmax(model_output).cpu().data.numpy() # digit probabilities\n",
    "        model_pred += np.argmax(model_probs, axis = 1).tolist() # digit predictions\n",
    "        targets += batch[1].numpy().tolist() # true digit values\n",
    "        \n",
    "    return sklearn.metrics.accuracy_score(targets, model_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters to try\n",
    "lam = 0.000001\n",
    "lrs = [0.1, 0.01]\n",
    "etas = [0.00001, 0.0001, 0.001, 0.01]\n",
    "gammas = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "num_epochs = 70\n",
    "\n",
    "input_size = 28*28 # mnist image sizes\n",
    "hidden_size = 50\n",
    "\n",
    "softmax = torch.nn.Softmax() # softmax to compute output probabilities \n",
    "loss_function = torch.nn.CrossEntropyLoss() # cross entropy loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for gamma=0.1, learning rate=0.1, eta=1e-05: 0.1064\n",
      "Validation accuracy for gamma=0.1, learning rate=0.1, eta=0.0001: 0.1064\n",
      "Validation accuracy for gamma=0.1, learning rate=0.1, eta=0.001: 0.1064\n",
      "Validation accuracy for gamma=0.1, learning rate=0.1, eta=0.01: 0.1064\n",
      "Validation accuracy for gamma=0.3, learning rate=0.1, eta=1e-05: 0.1064\n",
      "Validation accuracy for gamma=0.3, learning rate=0.1, eta=0.0001: 0.1064\n",
      "Validation accuracy for gamma=0.3, learning rate=0.1, eta=0.001: 0.1955\n",
      "Validation accuracy for gamma=0.3, learning rate=0.1, eta=0.01: 0.1064\n",
      "Validation accuracy for gamma=0.5, learning rate=0.1, eta=1e-05: 0.1064\n",
      "Validation accuracy for gamma=0.5, learning rate=0.1, eta=0.0001: 0.1064\n",
      "Validation accuracy for gamma=0.5, learning rate=0.1, eta=0.001: 0.1064\n",
      "Validation accuracy for gamma=0.5, learning rate=0.1, eta=0.01: 0.1064\n",
      "Validation accuracy for gamma=0.7, learning rate=0.1, eta=1e-05: 0.1064\n",
      "Validation accuracy for gamma=0.7, learning rate=0.1, eta=0.0001: 0.1064\n",
      "Validation accuracy for gamma=0.7, learning rate=0.1, eta=0.001: 0.1064\n",
      "Validation accuracy for gamma=0.7, learning rate=0.1, eta=0.01: 0.1064\n",
      "Validation accuracy for gamma=0.9, learning rate=0.1, eta=1e-05: 0.1064\n",
      "Validation accuracy for gamma=0.9, learning rate=0.1, eta=0.0001: 0.1064\n",
      "Validation accuracy for gamma=0.9, learning rate=0.1, eta=0.001: 0.1064\n",
      "Validation accuracy for gamma=0.9, learning rate=0.1, eta=0.01: 0.1064\n",
      "Validation accuracy for gamma=0.1, learning rate=0.01, eta=1e-05: 0.109\n",
      "Validation accuracy for gamma=0.1, learning rate=0.01, eta=0.0001: 0.0983\n",
      "Validation accuracy for gamma=0.1, learning rate=0.01, eta=0.001: 0.0915\n",
      "Validation accuracy for gamma=0.1, learning rate=0.01, eta=0.01: 0.0636\n",
      "Validation accuracy for gamma=0.3, learning rate=0.01, eta=1e-05: 0.1064\n",
      "Validation accuracy for gamma=0.3, learning rate=0.01, eta=0.0001: 0.1064\n",
      "Validation accuracy for gamma=0.3, learning rate=0.01, eta=0.001: 0.0915\n",
      "Validation accuracy for gamma=0.3, learning rate=0.01, eta=0.01: 0.103\n",
      "Validation accuracy for gamma=0.5, learning rate=0.01, eta=1e-05: 0.0967\n",
      "Validation accuracy for gamma=0.5, learning rate=0.01, eta=0.0001: 0.0991\n",
      "Validation accuracy for gamma=0.5, learning rate=0.01, eta=0.001: 0.0983\n",
      "Validation accuracy for gamma=0.5, learning rate=0.01, eta=0.01: 0.1009\n",
      "Validation accuracy for gamma=0.7, learning rate=0.01, eta=1e-05: 0.0967\n",
      "Validation accuracy for gamma=0.7, learning rate=0.01, eta=0.0001: 0.099\n",
      "Validation accuracy for gamma=0.7, learning rate=0.01, eta=0.001: 0.0991\n",
      "Validation accuracy for gamma=0.7, learning rate=0.01, eta=0.01: 0.0991\n",
      "Validation accuracy for gamma=0.9, learning rate=0.01, eta=1e-05: 0.0991\n",
      "Validation accuracy for gamma=0.9, learning rate=0.01, eta=0.0001: 0.099\n",
      "Validation accuracy for gamma=0.9, learning rate=0.01, eta=0.001: 0.103\n",
      "Validation accuracy for gamma=0.9, learning rate=0.01, eta=0.01: 0.103\n",
      "Test accuracy for gamma=0.3, learning rate=0.1, eta=0.001: 0.199\n"
     ]
    }
   ],
   "source": [
    "### noise used in gradients\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# loop through hyperparams\n",
    "best_val_acc = 0\n",
    "for lr in lrs:\n",
    "    for gamma in gammas:\n",
    "        for eta in etas:\n",
    "\n",
    "            # instantiate model\n",
    "            deep_FFNN = Deep_FFNN(input_size, hidden_size)\n",
    "\n",
    "            # SGD optimizer\n",
    "            optimizer = torch.optim.SGD(deep_FFNN.parameters(), lr, weight_decay = lam)\n",
    "\n",
    "            # for lessening noise over time\n",
    "            t = 0\n",
    "            for epoch in range(num_epochs):   \n",
    "                for batch in train_loader:\n",
    "                    model_output = deep_FFNN(Variable(torch.squeeze(batch[0], 1).view(len(batch[0]), input_size)).cuda()) # model predictions\n",
    "                    targets = Variable(batch[1]).cuda() # true digit values\n",
    "\n",
    "                    optimizer.zero_grad() # zero gradient\n",
    "                    loss_batch = loss_function(model_output, targets) # compute loss\n",
    "                    loss_batch.backward() # take the gradient wrt parameters\n",
    "                        \n",
    "                    sigma_t = np.sqrt(eta/((1 + t)**gamma)) # sigma for noise\n",
    "                    noise = torch.normal(means = torch.zeros(1), std = torch.ones(1) * sigma_t).numpy()[0].astype(np.float64) # get noise value\n",
    "\n",
    "                    for param in list(deep_FFNN.parameters()):\n",
    "                        param.grad += noise # add noise to gradient with respect to parameter\n",
    "                            \n",
    "                    optimizer.step() # update parameters\n",
    "                    t += 1\n",
    "                                    \n",
    "            # see validation results\n",
    "            val_acc = compute_accuracy(validation_loader, deep_FFNN, input_size)\n",
    "            print('Validation accuracy for gamma=' + str(gamma) + ', learning rate=' + str(lr) \n",
    "                  + ', eta=' + str(eta) + ': ' + str(val_acc))\n",
    "                \n",
    "            # save best model and best model hyperparams\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_lr = lr\n",
    "                best_gamma = gamma\n",
    "                best_eta = eta\n",
    "                best_model = deep_FFNN\n",
    "\n",
    "# see test results\n",
    "test_acc = compute_accuracy(test_loader, best_model, input_size)\n",
    "print('Test accuracy for gamma=' + str(best_gamma) + ', learning rate=' + str(best_lr) \n",
    "      + ', eta=' + str(best_eta) + ': ' + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for learning rate=0.1: 0.1064\n",
      "Validation accuracy for learning rate=0.01: 0.099\n",
      "Test accuracy for learning rate=0.01: 0.1135\n"
     ]
    }
   ],
   "source": [
    "### no noise used in gradients\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# loop through hyperparams\n",
    "best_val_acc = 0\n",
    "for lr in lrs:\n",
    "    \n",
    "    # instantiate model\n",
    "    deep_FFNN = Deep_FFNN(input_size, hidden_size)\n",
    "    \n",
    "    # SGD optimizer\n",
    "    optimizer = torch.optim.SGD(deep_FFNN.parameters(), lr, weight_decay = lam)\n",
    "\n",
    "    for epoch in range(num_epochs):   \n",
    "        for batch in train_loader:\n",
    "            model_output = deep_FFNN(Variable(torch.squeeze(batch[0], 1).view(len(batch[0]), input_size)).cuda()) # model predictions\n",
    "            targets = Variable(batch[1]).cuda() # true digit values\n",
    "\n",
    "            optimizer.zero_grad() # zero gradient\n",
    "            loss_batch = loss_function(model_output, targets) # compute loss\n",
    "            loss_batch.backward() # take the gradient wrt parameters\n",
    "            optimizer.step() # update parameters\n",
    "                        \n",
    "                \n",
    "    # see validation results\n",
    "    val_acc = compute_accuracy(validation_loader, deep_FFNN, input_size)\n",
    "    print('Validation accuracy for learning rate=' + str(lr) + ': ' + str(val_acc))\n",
    "            \n",
    "    # save best model and best model hyperparams\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_lr = lr\n",
    "        best_model = deep_FFNN\n",
    "\n",
    "# see test results\n",
    "test_acc = compute_accuracy(test_loader, best_model, input_size)\n",
    "print('Test accuracy for learning rate=' + str(lr) + ': ' + str(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5.3",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
