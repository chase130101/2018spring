{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Homework 2\n",
    "\n",
    "\n",
    "**Due Date: ** Friday, Febrary 9th, 2017 at 10am\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Upload your final answers as well as your iPython notebook containing all work to Canvas.\n",
    "\n",
    "- Structure your notebook and your work to maximize readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student: Christopher Hase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Monte Carlo Integration\n",
    "\n",
    "Let $X$ be a random variable with distribution described by the following pdf:\n",
    "\n",
    "$$\n",
    "f_X(x) = \\begin{cases}\n",
    "\\frac{1}{12}(x-1), &1\\leq x\\leq 3\\\\\n",
    "-\\frac{1}{12}(x-5), &3< x\\leq 5\\\\\n",
    "\\frac{1}{6}(x-5), &5< x\\leq 7\\\\\n",
    "-\\frac{1}{6}(x-9), &7< x\\leq 9\\\\\n",
    "0, &otherwise\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Let $h$ be the following function of $X$:\n",
    "\n",
    "$$\n",
    "h(X) = \\frac{1}{3\\sqrt{2}\\pi}\\mathrm{exp}\\left\\{ -\\frac{1}{18}\\left( X - 5\\right)^2\\right\\}\n",
    "$$\n",
    "\n",
    "Compute $\\mathbb{E}[h(X)]$ via Monte Carlo simulation using the following sampling methods:\n",
    "- inverse transform sampling\n",
    "- rejection sampling with both uniform proposal distribution and normal proposal distribution (steroids) (with appropriately chosen parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverse transform sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for computing the value of X at a given percentile in its PDF\n",
    "def inverse_CDF(u):\n",
    "    if u <= 1/6:\n",
    "        return 1 + (24 * u)**0.5\n",
    "    \n",
    "    elif u > 1/6 and u <= 1/3:\n",
    "        return 5 - 2 * (1 - 6 * (u - 1/6))**0.5\n",
    "    \n",
    "    elif u > 1/3 and u <= 2/3:\n",
    "        return 5 + 2 * (3 * (u - 1/3))**0.5\n",
    "    \n",
    "    elif u > 2/3 and u <= 1:\n",
    "        return 9 - 2 * (1 - 3 * (u - 2/3))**0.5\n",
    "    \n",
    "    \n",
    "# function to perform inverse transform sampling from fX\n",
    "def inverse_transform_samples_fX(num_samples):\n",
    "    samples = np.zeros(num_samples)\n",
    "    for i in range(num_samples):\n",
    "        u = random.random() # sampling from unif(0,1)\n",
    "        samples[i] = inverse_CDF(u) # computing value of x at the percentile of u in its PDF\n",
    "        \n",
    "    return samples\n",
    "    \n",
    "\n",
    "# function for computing the value of random variable h(X) given a value of the random variable X\n",
    "def hX(x):\n",
    "    return (1/(3 * np.pi * 2**0.5)) * np.exp((-(x - 5)**2)/18) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated mean of h(X) using inverse transform sampling: 0.0589811432951\n"
     ]
    }
   ],
   "source": [
    "random.seed(10)\n",
    "\n",
    "num_samples = 1000\n",
    "samples_inv_transform = inverse_transform_samples_fX(num_samples) # inverse transform sampling from fX\n",
    "h_values_inv_transform = hX(samples_inv_transform) # computing random variable h(X) at values of those samples\n",
    "print('Estimated mean of h(X) using inverse transform sampling: ' + str(np.mean(h_values_inv_transform)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rejection sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for computing the value PDF of X at x\n",
    "def fX(x):\n",
    "    if x >= 1 and x <= 3:\n",
    "        return (1/12) * (x - 1)\n",
    "    \n",
    "    elif x > 3 and x <= 5:\n",
    "        return (-1/12) * (x - 5)\n",
    "    \n",
    "    elif x > 5 and x <= 7:\n",
    "        return (1/6) * (x - 5)\n",
    "    \n",
    "    elif x > 7 and x <= 9:\n",
    "        return (-1/6) * (x - 9)\n",
    "    \n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "# function to perform rejection sampling from fX\n",
    "def rejection_samples_fX(proposal, num_samples, M, x_min = 1, x_max = 9, mean = 5, std = 2.5, func = fX):\n",
    "    samples = np.zeros(num_samples)\n",
    "    num_accepted = 0\n",
    "\n",
    "    while(num_accepted < num_samples):\n",
    "        if proposal == 'unif':\n",
    "            x = np.random.uniform(x_min, x_max) # sample from unif(x_min, x_max)\n",
    "            y = np.random.uniform(0, 1) # sample from unif(0, 1)\n",
    "\n",
    "            if y < func(x)/((1/(x_max - x_min)) * M): # condition for acceptance\n",
    "                samples[num_accepted] = x\n",
    "                num_accepted += 1\n",
    "                \n",
    "        if proposal == 'normal':\n",
    "            x = np.random.normal(loc = mean, scale = std) # sample from normal\n",
    "            if x >= x_min and x <= x_max:\n",
    "                y = np.random.uniform(0, 1) # sample from unif(0, 1)\n",
    "\n",
    "                if y < func(x)/(norm.pdf(x, loc = mean, scale = std) * M): # condition for acceptance\n",
    "                    samples[num_accepted] = x\n",
    "                    num_accepted += 1\n",
    "                \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated mean of h(X) using rejection sampling and uniform proposal distribution: 0.0589763007697\n",
      "Estimated mean of h(X) using rejection sampling and normal proposal distribution: 0.0589816169223\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "num_samples = 1000\n",
    "samples_unif_prop = rejection_samples_fX('unif', num_samples, 5) # rejection sampling from fX with uniform proposal distribution\n",
    "h_values_unif_prop = hX(samples_unif_prop) # computing random variable h(X) at values of those samples\n",
    "print('Estimated mean of h(X) using rejection sampling and uniform proposal distribution: ' + str(np.mean(h_values_unif_prop)))\n",
    "\n",
    "samples_norm_prop = rejection_samples_fX('normal', num_samples, 5) # rejection sampling from fX with normal proposal distribution\n",
    "h_values_norm_prop = hX(samples_norm_prop) # computing random variable h(X) at values of those samples\n",
    "print('Estimated mean of h(X) using rejection sampling and normal proposal distribution: ' + str(np.mean(h_values_norm_prop)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Variance Reduction\n",
    "\n",
    "### Part A\n",
    "\n",
    "Compute the variance of each estimate of $\\mathbb{E}[h(X)]$ obtained in Problem 1. What do you see?\n",
    "\n",
    "### Part B (Stratified Sampling)\n",
    "\n",
    "Often, a complex integral can be computed with more ease if one can break up the domain of the integral into pieces and if on each piece of the domain the integral is simplified. \n",
    "\n",
    "- Find a natural way to divide the domain of $X$ and express $\\mathbb{E}[h(X)]$ as an ***correctly*** weighted sum of integrals over the pieces of the domain of $X$. (This constitutes the essentials of Stratified Sampling)\n",
    "\n",
    "- Estimate each integral in the summand using rejection sampling using a normal proposal distribution (with sensibly chosen parameters). From these, estimate $\\mathbb{E}[h(X)]$.\n",
    "\n",
    "- Compute the variance of your estimate of $\\mathbb{E}[h(X)]$. Compare with the variance of your previous estimate of $\\mathbb{E}[h(X)]$ (in Part A, using rejection sampling, a normal proposal distribution over the entire domain of $X$).\n",
    "\n",
    "Read more about Stratified Sampling:\n",
    "\n",
    "1. [Monte Carlo Methods](http://www.public.iastate.edu/~mervyn/stat580/Notes/s09mc.pdf)\n",
    "\n",
    "2. [Variance Reduction Techniques Chapter](http://sas.uwaterloo.ca/~dlmcleis/s906/chapt4.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated variance of sample mean of h(X) when using inverse transform sampling: 1.01104695622e-07\n",
      "Estimated variance of sample mean of h(X) when using rejection sampling with uniform proposal distribution: 1.10497011341e-07\n",
      "Estimated variance of sample mean of h(X) when using rejection sampling with normal proposal distribution: 9.91070320052e-08\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "num_means = 1000\n",
    "est_means_h_inv = np.zeros(num_means)\n",
    "est_means_h_rej_unif = np.zeros(num_means)\n",
    "est_means_h_rej_norm = np.zeros(num_means)\n",
    "\n",
    "num_samples = 1000\n",
    "for i in range(num_means):\n",
    "    samples_inv_transform = inverse_transform_samples_fX(num_samples) # inverse transform sampling from fX\n",
    "    h_values_inv_transform = hX(samples_inv_transform) # computing random variable h(X) at values of those samples\n",
    "    est_means_h_inv[i] = np.mean(h_values_inv_transform) # estimating mean of h(X) from sample\n",
    "    \n",
    "    samples_rej_unif = rejection_samples_fX('unif', num_samples, 5) # rejection sampling from fX with uniform proposal distribution\n",
    "    h_values_rej_unif = hX(samples_rej_unif) # computing random variable h(X) at values of those samples\n",
    "    est_means_h_rej_unif[i] = np.mean(h_values_rej_unif) # estimating mean of h(X) from sample\n",
    "\n",
    "    samples_rej_norm = rejection_samples_fX('normal', num_samples, 5) # rejection sampling from fX with normal proposal distribution\n",
    "    h_values_rej_norm = hX(samples_rej_norm) # computing random variable h(X) at values of those samples\n",
    "    est_means_h_rej_norm[i] = np.mean(h_values_rej_norm) # estimating mean of h(X) from sample\n",
    "    \n",
    "print('Estimated variance of sample mean of h(X) when using inverse transform sampling: ' + str(np.var(est_means_h_inv, ddof = 1)))\n",
    "print('Estimated variance of sample mean of h(X) when using rejection sampling with uniform proposal distribution: ' + str(np.var(est_means_h_rej_unif, ddof = 1)))\n",
    "print('Estimated variance of sample mean of h(X) when using rejection sampling with normal proposal distribution: ' + str(np.var(est_means_h_rej_norm, ddof = 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated variances of the sample mean of h(X) are very close in magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The domain of $X$ is naturally divided into 4 parts based on its PDF $f_X$.\n",
    "\n",
    "$E[h(X)] = P(1\\leq X\\leq 3)\\displaystyle\\int_1^3h(x)\\cdot\\dfrac{1}{12}(x-1) dx+P(3< X\\leq 5)\\displaystyle\\int_3^5h(x)\\cdot -\\dfrac{1}{12}(x-5)dx+P(5< X\\leq 7)\\displaystyle\\int_5^7h(x)\\cdot \\dfrac{1}{6}(x-5)dx+P(7< X\\leq 9)\\displaystyle\\int_7^9h(x)\\cdot -\\dfrac{1}{6}(x-9)dx\\\\\\\\\n",
    "=\\dfrac{1}{6}\\cdot\\displaystyle\\int_1^3h(x)\\cdot\\dfrac{1}{12}(x-1) dx+\\dfrac{1}{6}\\cdot\\displaystyle\\int_3^5h(x)\\cdot -\\dfrac{1}{12}(x-5)dx+\\dfrac{1}{3}\\cdot\\displaystyle\\int_5^7h(x)\\cdot \\dfrac{1}{6}(x-5)dx+\\dfrac{1}{3}\\cdot\\displaystyle\\int_7^9h(x)\\cdot -\\dfrac{1}{6}(x-9)dx$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimate of 1st integral: 0.0500896557542\n",
      "Estimate of 2nd integral: 0.0673530001159\n",
      "Estimate of 3rd integral: 0.0680925384897\n",
      "Estimate of 4th integral: 0.0497164409828\n",
      "Estimated mean of h(X) using stratified sampling: 0.0588434358025\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "num_samples = 1000\n",
    "\n",
    "samples_13 = rejection_samples_fX('normal', num_samples, M = 2, x_min = 1, x_max = 3, mean = 2, std = 0.4, func = lambda x: (1/12) * (x - 1)) # rejection sampling from first stratum\n",
    "h_values_13 = hX(samples_13) # computing random variable h(X) at values of those samples\n",
    "print('Estimate of 1st integral: ' + str(np.mean(h_values_13)))\n",
    "\n",
    "samples_35 = rejection_samples_fX('normal', num_samples, M = 2, x_min = 3, x_max = 5, mean = 4, std = 0.4, func = lambda x: (-1/12) * (x - 5)) # rejection sampling from seoond stratum\n",
    "h_values_35 = hX(samples_35) # computing random variable h(X) at values of those samples\n",
    "print('Estimate of 2nd integral: ' + str(np.mean(h_values_35)))\n",
    "\n",
    "samples_57 = rejection_samples_fX('normal', num_samples, M = 2, x_min = 5, x_max = 7, mean = 6, std = 0.4, func = lambda x: (1/6) * (x - 5)) # rejection sampling from third stratum\n",
    "h_values_57 = hX(samples_57) # computing random variable h(X) at values of those samples\n",
    "print('Estimate of 3rd integral: ' + str(np.mean(h_values_57)))\n",
    "\n",
    "samples_79 = rejection_samples_fX('normal', num_samples, M = 2, x_min = 7, x_max = 9, mean = 8, std = 0.4, func = lambda x: (-1/6) * (x - 9)) # rejection sampling from fourth stratum\n",
    "h_values_79 = hX(samples_79) # computing random variable h(X) at values of those samples\n",
    "print('Estimate of 4th integral: ' + str(np.mean(h_values_79)))\n",
    "\n",
    "# weighted sum of integrals to estimate mean of h(X)\n",
    "hX_est_mean = (1/6) * np.mean(h_values_13) + (1/6) * np.mean(h_values_35) + (1/3) * np.mean(h_values_57) + (1/3) * np.mean(h_values_79)\n",
    "\n",
    "print('Estimated mean of h(X) using stratified sampling: ' + str(hX_est_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated variance of weighted mean of h(X) when using stratified sampling: 9.47500939147e-09\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "num_means = 100\n",
    "est_means_h_strat = np.zeros(num_means)\n",
    "\n",
    "num_samples = 1000\n",
    "for i in range(num_means):\n",
    "    samples_13 = rejection_samples_fX('normal', num_samples, M = 2, x_min = 1, x_max = 3, mean = 2, std = 0.4, func = lambda x: (1/12) * (x - 1)) # rejection sampling from first stratum\n",
    "    h_values_13 = hX(samples_13) # computing random variable h(X) at values of those samples\n",
    "\n",
    "    samples_35 = rejection_samples_fX('normal', num_samples, M = 2, x_min = 3, x_max = 5, mean = 4, std = 0.4, func = lambda x: (-1/12) * (x - 5)) # rejection sampling from seoond stratum\n",
    "    h_values_35 = hX(samples_35) # computing random variable h(X) at values of those samples\n",
    "\n",
    "    samples_57 = rejection_samples_fX('normal', num_samples, M = 2, x_min = 5, x_max = 7, mean = 6, std = 0.4, func = lambda x: (1/6) * (x - 5)) # rejection sampling from third stratum\n",
    "    h_values_57 = hX(samples_57) # computing random variable h(X) at values of those samples\n",
    "\n",
    "    samples_79 = rejection_samples_fX('normal', num_samples, M = 2, x_min = 7, x_max = 9, mean = 8, std = 0.4, func = lambda x: (-1/6) * (x - 9)) # rejection sampling from fourth stratum\n",
    "    h_values_79 = hX(samples_79) # computing random variable h(X) at values of those samples\n",
    "\n",
    "    # weighted sum of integrals to estimate mean of h(X)\n",
    "    est_means_h_strat[i] = (1/6) * np.mean(h_values_13) + (1/6) * np.mean(h_values_35) + (1/3) * np.mean(h_values_57) + (1/3) * np.mean(h_values_79)\n",
    "    \n",
    "print('Estimated variance of weighted mean of h(X) when using stratified sampling: ' + str(np.var(est_means_h_strat, ddof = 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated variance of the weighted mean of $h(X)$ computed using stratified sampling with normal proposal distributions is much lower than the estimated variance of the sample mean of $h(X)$ computed using rejection sampling with a single normal proposal distribution over the entire domain of $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Linear Regression\n",
    "\n",
    "Consider the following base Regression class, which roughly follows the API in the python package `scikit-learn`.\n",
    "\n",
    "Our model is the the multivariate linear model whose MLE solution or equivalent cost minimization was talked about in lecture:\n",
    "\n",
    "$$y = X\\beta + \\epsilon$$\n",
    "where $y$ is a length $n$ vector, $X$ is an $n \\times p$ matrix created by stacking the features for each data point, and $\\beta$ is a $p$ length vector of coefficients.\n",
    "\n",
    "The class showcases the API:\n",
    "\n",
    "$fit(X, y)$: Fits linear model to $X$ and $y$.\n",
    "\n",
    "$get\\_params()$: Returns $\\hat{\\beta}$ for the fitted model. The parameters should be stored in a dictionary with keys \"intercept\" and \"coef\" that give us $\\hat{\\beta_0}$ and $\\hat{\\beta_{1:}}$. (The second value here is thus a numpy array of coefficient values)\n",
    "\n",
    "$predict(X)$: Predict new values with the fitted model given $X$.\n",
    "\n",
    "$score(X, y)$: Returns $R^2$ value of the fitted model.\n",
    "\n",
    "$set\\_params()$: Manually set the parameters of the linear model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Regression(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.params = dict()\n",
    "    \n",
    "    \n",
    "    def get_params(self, k):\n",
    "        return self.params[k]\n",
    "    \n",
    "    \n",
    "    def set_params(self, **kwargs):\n",
    "        for k,v in kwargs.iteritems():\n",
    "            self.params[k] = v\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        \n",
    "    def score(self, X, y):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A:  a class for Ordinary Least Squares\n",
    "\n",
    "Inherit from this class to create an ordinary Least Squares Linear Regression class. \n",
    "\n",
    "It's signature will look like this:\n",
    "\n",
    "`class OLS(Regression):`\n",
    "\n",
    "Implement `fit`, `predict` and `score`. This will involve some linear algebra. (You might want to read up on pseudo-inverses before you directly implement the linear algebra on the lecure slides).\n",
    "\n",
    "#### $R^2$ score\n",
    "\n",
    "To implement `score`, look below:\n",
    "\n",
    "The $R^2$ score is defined as: $${R^{2} = {1-{SS_E \\over SS_T}}}$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$SS_T=\\sum_i (y_i-\\bar{y})^2, SS_R=\\sum_i (\\hat{y_i}-\\bar{y})^2, SS_E=\\sum_i (y_i - \\hat{y_i})^2$$\n",
    "where  ${y_i}$ are the original data values, $\\hat{y_i}$ are the predicted values, and $\\bar{y_i}$ is the mean of the original data values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OLS(Regression):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.params = dict()\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # adding column of ones to X matrix to fit bias term if X matrix doesn't already have a column of ones\n",
    "        if np.sum(X[:, 0] != np.ones(X.shape[0])) == X.shape[0]:\n",
    "            X_ones = np.concatenate((np.ones((X.shape[0], 1)), X), axis = 1)\n",
    "        else:\n",
    "            X_ones = X\n",
    "        \n",
    "        betas = np.linalg.pinv(X_ones.T.dot(X_ones)).dot(X_ones.T).dot(y) # ordinary least squares solution\n",
    "        self.params['intercept'] = betas[0]\n",
    "        self.params['coef'] = betas[1:]\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        # adding column of ones to X matrix if X matrix doesn't already have a column of ones\n",
    "        if np.sum(X[:, 0] != np.ones(X.shape[0])) == X.shape[0]:\n",
    "            X_ones = np.concatenate((np.ones((X.shape[0], 1)), X), axis = 1)\n",
    "        else:\n",
    "            X_ones = X\n",
    "        \n",
    "        # putting intercept and coefficients into a single column vector\n",
    "        betas = np.concatenate((np.array(self.params['intercept']).reshape(1, -1), self.params['coef'].reshape(len(self.params['coef']), -1)), axis = 0)\n",
    "                               \n",
    "        return np.squeeze(X_ones.dot(betas), axis = 1)\n",
    "    \n",
    "        \n",
    "    def score(self, X, y):\n",
    "        pred = self.predict(X)\n",
    "        sse = np.sum((y - pred)**2)\n",
    "        sst = np.sum((y - np.mean(y))**2)\n",
    "        \n",
    "        return 1 - sse/sst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: test your code\n",
    "\n",
    "We'll create a synthetic data set using the code below. (Read the documentation for `make_regression` to see what is going on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 76.6568183 ,  77.67682678,  63.78807738,  19.3299907 ,\n",
       "        59.01638708,  53.13633737,  28.77629958,  10.01888939,\n",
       "         9.25346811,  59.55220395])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(99)\n",
    "X, y, coef = make_regression(30, 10, 10, bias=1, noise=2, coef=True)\n",
    "coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that your code recovers these coefficients approximately on doing the fit. Plot the predicted `y` against the actual `y`. Also calculate the score using the same sets `X` and `y`. The usage will look something like:\n",
    "\n",
    "```python\n",
    "lr = OLS()\n",
    "lr.fit(X, y)\n",
    "lr.get_params['coef']\n",
    "lr.predict(X)\n",
    "lr.score(X, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEWCAYAAACnlKo3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzMwKcxIxM7PCnETMzKwwJxEzMyvs/wPmYJ66vbaY1gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12c2c76be80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 1.34580357078\n",
      "\n",
      "Coefficients: [ 77.20719705  76.51004831  62.97865316  18.4436452   58.50019885\n",
      "  53.25126559  28.29088241   9.33333359  10.29584457  59.1606719 ]\n",
      "\n",
      "Predictions: [  48.57564537   24.85508406  246.39920911   64.72282184  124.00042911\n",
      " -266.57653702  118.15510334 -108.57077603  191.15229644  174.74404249\n",
      " -103.59066227  -59.1576374   -54.70947468   73.91003582  505.22781806\n",
      "   39.53820436 -191.02175593 -201.71787963   46.1500923  -111.90307749\n",
      "  117.38777883 -111.52335297   41.66543625  168.33074858   73.41934029\n",
      "  -80.64319083  155.12182695  -94.18157131  -48.17239883   40.78548557]\n",
      "\n",
      "Score (R^2): 0.999915583206\n"
     ]
    }
   ],
   "source": [
    "# fitting and predicting\n",
    "lr = OLS()\n",
    "lr.fit(X, y)\n",
    "pred = lr.predict(X)\n",
    "\n",
    "# plotting pred y vs. true y\n",
    "plt.plot(y, pred, 'bo')\n",
    "plt.xlabel('True Y')\n",
    "plt.ylabel('Predicted Y')\n",
    "plt.title('Predicted Y vs. True Y')\n",
    "plt.show()\n",
    "\n",
    "# displaying values of interest\n",
    "print('Intercept: ' + str(lr.get_params('intercept')))\n",
    "print()\n",
    "print('Coefficients: ' + str(lr.get_params('coef')))\n",
    "print()\n",
    "print('Predictions: ' + str(pred))\n",
    "print()\n",
    "print('Score (R^2): ' + str(lr.score(X, y)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
