{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APMTH 207: Advanced Scientific Computing: \n",
    "## Stochastic Methods for Data Analysis, Inference and Optimization\n",
    "## Homework #4\n",
    "**Harvard University**<br>\n",
    "**Spring 2017**<br>\n",
    "**Instructors: Rahul Dave**<br>\n",
    "**Due Date: ** Friday, Febrary 23rd, 2018 at 11:00am\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Upload your final answers as well as your iPython notebook containing all work to Canvas.\n",
    "\n",
    "- Structure your notebook and your work to maximize readability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Christopher Hase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import sklearn.metrics\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Optimization (contd)\n",
    "\n",
    "Suppose you are building a pricing model for laying down telecom cables over a geographical region. Your model takes as input a pair of  coordinates, $(x, y)$, and contains two parameters, $\\lambda_1, \\lambda_2$. Given a coordinate, $(x, y)$, and model parameters, the loss in revenue corresponding to the price model at location $(x, y)$ is described by\n",
    "$$\n",
    "L(x, y, \\lambda_1, \\lambda_2) = 0.000045\\lambda_2^2 y - 0.000098\\lambda_1^2 x  + 0.003926\\lambda_1 x\\exp\\left\\{\\left(y^2 - x^2\\right)\\left(\\lambda_1^2 + \\lambda_2^2\\right)\\right\\}\n",
    "$$\n",
    "Read the data contained in `HW3_data.csv`. This is a set of coordinates configured on the curve $y^2 - x^2 = -0.1$. Given the data, find parameters $\\lambda_1, \\lambda_2$ that minimize the net loss over the entire dataset.\n",
    "\n",
    "### Part A: Further problems with descent algorithms\n",
    "Using your implementation of gradient descent and stochastic gradient descent, document the behaviour of your two algorithms for the following starting points, and for a number of stepsizes of your choice:\n",
    "- $(\\lambda_1, \\lambda_2) = (-2.47865, 0)$\n",
    "- $(\\lambda_1, \\lambda_2) = (-3, 0)$\n",
    "- $(\\lambda_1, \\lambda_2) = (-5, 0)$\n",
    "- $(\\lambda_1, \\lambda_2) = (-10, 0)$\n",
    "Based on your analysis of the loss function $L$, explain what is happening to your descent algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to compute loss\n",
    "def loss(x, y, lam1, lam2):\n",
    "    return 0.000045 * lam2**2 * y - 0.000098 * lam1**2 * x + 0.003926 * lam1 * x * np.exp((-0.1) * (lam1**2 + lam2**2))\n",
    "\n",
    "# function to compute derivative of loss with respect to lam1\n",
    "def derivative_loss_lam1(x, y, lam1, lam2):\n",
    "    return -0.000196 * lam1 * x + x * (0.003926 - 0.007852 * lam1**2 * (x**2 - y**2)) * np.exp((-0.1) * (lam1**2 + lam2**2))\n",
    "\n",
    "# function to compute derivative of loss with respect to lam2\n",
    "def derivative_loss_lam2(x, y, lam1, lam2):\n",
    "    return 0.00009 * lam2 * y - 0.007852 * lam1 * lam2 * x * (0.1) * np.exp((-0.1) * (lam1**2 + lam2**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in and manipulate data\n",
    "hw3_data = pd.read_csv('HW3_data.csv', header = None)\n",
    "hw3_data = hw3_data.transpose()\n",
    "hw3_data.columns = ['x', 'y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda 1 (start): -2.47865\n",
      "Lambda 1(end): -2.47865012747\n",
      "Lambda 2 (start): 0\n",
      "Lambda 2 (end): 0.0\n",
      "Learning rate: 0.01\n",
      "Net loss: 11.9545803086\n",
      "\n",
      "Lambda 1 (start): -2.47865\n",
      "Lambda 1(end): -2.47865001275\n",
      "Lambda 2 (start): 0\n",
      "Lambda 2 (end): 0.0\n",
      "Learning rate: 0.001\n",
      "Net loss: 11.9545803086\n",
      "\n",
      "Lambda 1 (start): -2.47865\n",
      "Lambda 1(end): -2.47865000127\n",
      "Lambda 2 (start): 0\n",
      "Lambda 2 (end): 0.0\n",
      "Learning rate: 0.0001\n",
      "Net loss: 11.9545803086\n",
      "\n",
      "Lambda 1 (start): -3\n",
      "Lambda 1(end): -5.36146740022\n",
      "Lambda 2 (start): 0\n",
      "Lambda 2 (end): 0.0\n",
      "Learning rate: 0.01\n",
      "Net loss: 8.16153144632\n",
      "\n",
      "Lambda 1 (start): -3\n",
      "Lambda 1(end): -5.35748731018\n",
      "Lambda 2 (start): 0\n",
      "Lambda 2 (end): 0.0\n",
      "Learning rate: 0.001\n",
      "Net loss: 8.16155742623\n",
      "\n",
      "Lambda 1 (start): -3\n",
      "Lambda 1(end): -5.34499093332\n",
      "Lambda 2 (start): 0\n",
      "Lambda 2 (end): 0.0\n",
      "Learning rate: 0.0001\n",
      "Net loss: 8.16181724474\n",
      "\n",
      "Lambda 1 (start): -5\n",
      "Lambda 1(end): -5.361462573\n",
      "Lambda 2 (start): 0\n",
      "Lambda 2 (end): 0.0\n",
      "Learning rate: 0.01\n",
      "Net loss: 8.16153146122\n",
      "\n",
      "Lambda 1 (start): -5\n",
      "Lambda 1(end): -5.35748776483\n",
      "Lambda 2 (start): 0\n",
      "Lambda 2 (end): 0.0\n",
      "Learning rate: 0.001\n",
      "Net loss: 8.1615574217\n",
      "\n",
      "Lambda 1 (start): -5\n",
      "Lambda 1(end): -5.34499112836\n",
      "Lambda 2 (start): 0\n",
      "Lambda 2 (end): 0.0\n",
      "Learning rate: 0.0001\n",
      "Net loss: 8.16181723858\n",
      "\n",
      "Lambda 1 (start): -10\n",
      "Lambda 1(end): -5.3650494718\n",
      "Lambda 2 (start): 0\n",
      "Lambda 2 (end): 0.0\n",
      "Learning rate: 0.01\n",
      "Net loss: 8.16153150291\n",
      "\n",
      "Lambda 1 (start): -10\n",
      "Lambda 1(end): -5.36901517619\n",
      "Lambda 2 (start): 0\n",
      "Lambda 2 (end): 0.0\n",
      "Learning rate: 0.001\n",
      "Net loss: 8.1615574567\n",
      "\n",
      "Lambda 1 (start): -10\n",
      "Lambda 1(end): -5.38153518796\n",
      "Lambda 2 (start): 0\n",
      "Lambda 2 (end): 0.0\n",
      "Learning rate: 0.0001\n",
      "Net loss: 8.16181782805\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# initializing lam1 and lam2\n",
    "lam1s = [-2.47865, -3, -5, -10]\n",
    "lam2 = 0\n",
    "\n",
    "lrs = [0.01, 0.001, 0.0001] # learning rate\n",
    "\n",
    "for i in range(len(lam1s)):\n",
    "    for lr in lrs:\n",
    "\n",
    "        loss_current = 0\n",
    "        \n",
    "        lam1_current = lam1s[i]\n",
    "        lam2_current = lam2\n",
    "        \n",
    "        # stop updating lambdas when loss 'barely' changes\n",
    "        while abs(loss_current - np.sum(loss(hw3_data['x'].values, hw3_data['y'].values, lam1_current, lam2_current))) > 0.0000001:\n",
    "            loss_current = np.sum(loss(hw3_data['x'].values, hw3_data['y'].values, lam1_current, lam2_current))\n",
    "            num_iter += 1\n",
    "\n",
    "            lam1_current -= lr * np.sum(derivative_loss_lam1(hw3_data['x'].values, hw3_data['y'].values, lam1_current, lam2_current)) # update lam1\n",
    "            lam2_current -= lr * np.sum(derivative_loss_lam2(hw3_data['x'].values, hw3_data['y'].values, lam1_current, lam2_current)) # update lam2\n",
    "        \n",
    "        print('Lambda 1 (start): ' + str(lam1s[i]))\n",
    "        print('Lambda 1(end): ' + str(lam1_current))\n",
    "        print('Lambda 2 (start): ' + str(lam2))\n",
    "        print('Lambda 2 (end): ' + str(lam2_current))\n",
    "        print('Learning rate: ' + str(lr))\n",
    "        print('Net loss: ' + str(np.sum(loss(hw3_data['x'].values, hw3_data['y'].values, lam1_current, lam2_current))))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under none of the initial conditions for $\\lambda_1$ and $\\lambda_2$ is gradient descent converging to the optimal values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda 1 (start): -2.47865\n",
      "Lambda 1(end): -2.4786327669\n",
      "Lambda 2 (start): 0\n",
      "Lambda 2 (end): 0.0\n",
      "Learning rate: 0.01\n",
      "Net loss: 11.9545803083\n",
      "\n",
      "Lambda 1 (start): -2.47865\n",
      "Lambda 1(end): -2.47864827669\n",
      "Lambda 2 (start): 0\n",
      "Lambda 2 (end): 0.0\n",
      "Learning rate: 0.001\n",
      "Net loss: 11.9545803086\n",
      "\n",
      "Lambda 1 (start): -2.47865\n",
      "Lambda 1(end): -2.47864982767\n",
      "Lambda 2 (start): 0\n",
      "Lambda 2 (end): 0.0\n",
      "Learning rate: 0.0001\n",
      "Net loss: 11.9545803086\n",
      "\n",
      "Lambda 1 (start): -3\n",
      "Lambda 1(end): -2.25237788748e+153\n",
      "Lambda 2 (start): 0\n",
      "Lambda 2 (end): 0.0\n",
      "Learning rate: 0.01\n",
      "Net loss: nan\n",
      "\n",
      "Lambda 1 (start): -3\n",
      "Lambda 1(end): -2.47870502812\n",
      "Lambda 2 (start): 0\n",
      "Lambda 2 (end): 0.0\n",
      "Learning rate: 0.001\n",
      "Net loss: 11.9545803027\n",
      "\n",
      "Lambda 1 (start): -3\n",
      "Lambda 1(end): -2.47932339477\n",
      "Lambda 2 (start): 0\n",
      "Lambda 2 (end): 0.0\n",
      "Learning rate: 0.0001\n",
      "Net loss: 11.9545795291\n",
      "\n",
      "Lambda 1 (start): -5\n",
      "Lambda 1(end): -4.65885964931e+153\n",
      "Lambda 2 (start): 0\n",
      "Lambda 2 (end): 0.0\n",
      "Learning rate: 0.01\n",
      "Net loss: nan\n",
      "\n",
      "Lambda 1 (start): -5\n",
      "Lambda 1(end): -2.47871668137\n",
      "Lambda 2 (start): 0\n",
      "Lambda 2 (end): 0.0\n",
      "Learning rate: 0.001\n",
      "Net loss: 11.9545803002\n",
      "\n",
      "Lambda 1 (start): -5\n",
      "Lambda 1(end): -2.48039219998\n",
      "Lambda 2 (start): 0\n",
      "Lambda 2 (end): 0.0\n",
      "Learning rate: 0.0001\n",
      "Net loss: 11.9545751273\n",
      "\n",
      "Lambda 1 (start): -10\n",
      "Lambda 1(end): -5.40822597497e+153\n",
      "Lambda 2 (start): 0\n",
      "Lambda 2 (end): 0.0\n",
      "Learning rate: 0.01\n",
      "Net loss: nan\n",
      "\n",
      "Lambda 1 (start): -10\n",
      "Lambda 1(end): -2.39199754164e+153\n",
      "Lambda 2 (start): 0\n",
      "Lambda 2 (end): 0.0\n",
      "Learning rate: 0.001\n",
      "Net loss: nan\n",
      "\n",
      "Lambda 1 (start): -10\n",
      "Lambda 1(end): -5.3648000799\n",
      "Lambda 2 (start): 0\n",
      "Lambda 2 (end): 0.0\n",
      "Learning rate: 0.0001\n",
      "Net loss: 8.16153077996\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# initializing lam1 and lam2\n",
    "lam1s = [-2.47865, -3, -5, -10]\n",
    "lam2 = 0\n",
    "\n",
    "lrs = [0.01, 0.001, 0.0001] # learning rate\n",
    "\n",
    "for i in range(len(lam1s)):\n",
    "    for lr in lrs:\n",
    "        \n",
    "        loss_current = 0\n",
    "        row = 0\n",
    "\n",
    "        lam1_current = lam1s[i]\n",
    "        lam2_current = lam2\n",
    "\n",
    "        # stop updating lambdas when loss 'barely' changes\n",
    "        while abs(loss_current - np.sum(loss(hw3_data['x'].values, hw3_data['y'].values, lam1_current, lam2_current))) > 0.0000001:\n",
    "            loss_current = np.sum(loss(hw3_data['x'].values, hw3_data['y'].values, lam1_current, lam2_current))\n",
    "\n",
    "            lam1_current -= lr * len(hw3_data) * derivative_loss_lam1(hw3_data['x'].values[row], hw3_data['y'].values[row], lam1_current, lam2_current) # update lam1\n",
    "            lam2_current -= lr * len(hw3_data) * derivative_loss_lam2(hw3_data['x'].values[row], hw3_data['y'].values[row], lam1_current, lam2_current) # update lam2\n",
    "\n",
    "\n",
    "            row += 1\n",
    "            if row == len(hw3_data): # condition indicating end of dataset; need to reset\n",
    "                row = 0\n",
    "\n",
    "        print('Lambda 1 (start): ' + str(lam1s[i]))\n",
    "        print('Lambda 1(end): ' + str(lam1_current))\n",
    "        print('Lambda 2 (start): ' + str(lam2))\n",
    "        print('Lambda 2 (end): ' + str(lam2_current))\n",
    "        print('Learning rate: ' + str(lr))\n",
    "        print('Net loss: ' + str(np.sum(loss(hw3_data['x'].values, hw3_data['y'].values, lam1_current, lam2_current))))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under none of the initial conditions for $\\lambda_1$ and $\\lambda_2$ is stochastic gradient descent converging to the optimal values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis of loss function and why descent algos behaving this way..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Logistic Regression and MNIST (contd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The MNIST dataset is one of the classic datasets in Machine Learning and is often one of the first datasets against which new classification algorithms test themselves. It consists of 70,000 images of handwritten digits, each of which is 28x28 pixels. \n",
    "\n",
    "Last time you used PyTorch to build a handwritten digit multi-class logistic regression classifier that you trained and tested with MNIST dataset.\n",
    "\n",
    "We'll introduce validation sets and regularization in this problem.\n",
    "\n",
    "Using the softmax formulation, write a PyTorch model that computes the cost function using an L2 regularization approach (see `optim.SGD` in PyTorch or write your own cost function) and minimizes the resulting cost function using mini-batch stochastic gradient descent.\n",
    "\n",
    "Construct and train your classifier using a batch size of 256 examples, a learning rate η=0.1, and a regularization factor λ=0.01.\n",
    "\n",
    "1. Using classification accuracy, evaluate how well your model is performing on the validation set at the end of each epoch. Plot this validation accuracy as the model trains. \n",
    "2. Duplicate this plot for some other values of the regularization parameter $\\lambda$. When should you stop the training for different values of λ? Give an approximate answer supported by using the plots.\n",
    "3. Select what you consider the best regularization parameter and predict the labels of the test set. Compare with the given labels. What classification accuracy do you obtain on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions/classes taken from: https://gist.github.com/t-vi/9f6118ff84867e89f3348707c7a1271f\n",
    "# to help create validation set\n",
    "\n",
    "class PartialDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, parent_ds, offset, length):\n",
    "        self.parent_ds = parent_ds\n",
    "        self.offset = offset\n",
    "        self.length = length\n",
    "        assert len(parent_ds)>=offset+length, Exception(\"Parent Dataset not long enough\")\n",
    "        super(PartialDataset, self).__init__()\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    def __getitem__(self, i):\n",
    "        return self.parent_ds[i+self.offset]\n",
    "\n",
    "def validation_split(dataset, val_share=0.1):\n",
    "    \"\"\"\n",
    "       Split a (training and vaidation combined) dataset into training and validation.\n",
    "       Note that to be statistically sound, the items in the dataset should be statistically\n",
    "       independent (e.g. not sorted by class, not several instances of the same dataset that\n",
    "       could end up in either set).\n",
    "    \n",
    "       inputs:\n",
    "          dataset:   (\"training\") dataset to split into training and validation\n",
    "          val_share: fraction of validation data (should be 0<val_share<1, default: 0.1)\n",
    "       returns: input dataset split into test_ds, val_ds\n",
    "       \n",
    "       \"\"\"\n",
    "    val_offset = int(len(dataset)*(1-val_share))\n",
    "    return PartialDataset(dataset, 0, val_offset), PartialDataset(dataset, val_offset, len(dataset)-val_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "# help from: https://gist.github.com/xmfbit/b27cdbff68870418bdb8cefa86a2d558 \n",
    "# for data loading\n",
    "\n",
    "# load in MNIST\n",
    "train_validation = torchvision.datasets.MNIST(root = os.getcwd(), train=True, transform=torchvision.transforms.ToTensor(), download = True)\n",
    "train, validation = validation_split(train_validation, 1/6) # get train/validation split\n",
    "test = torchvision.datasets.MNIST(root = os.getcwd(), train=False, transform=torchvision.transforms.ToTensor(), download = True)\n",
    "\n",
    "# create batches\n",
    "batch_size = 256\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train, batch_size = batch_size, shuffle = True)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset = validation, batch_size = batch_size, shuffle = False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for computing accuracy\n",
    "def compute_accuracy(data_loader, model):\n",
    "    model_pred = []\n",
    "    targets = []\n",
    "    for batch in data_loader:\n",
    "        model_output = model(Variable(torch.squeeze(batch[0], 1).view(len(batch[0]), 28*28))) # output from linear layer\n",
    "        model_probs = softmax(model_output).data.numpy() # digit probabilities\n",
    "        model_pred += np.argmax(model_probs, axis = 1).tolist() # digit predictions\n",
    "        targets += batch[1].numpy().tolist() # true digit values\n",
    "        \n",
    "    return sklearn.metrics.accuracy_score(targets, model_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "multiclass_logit = torch.nn.Linear(in_features = 28*28, out_features = 10, bias = True) # linear layer\n",
    "softmax = torch.nn.Softmax() # softmax to compute output probabilities \n",
    "loss_function = torch.nn.CrossEntropyLoss() # cross entropy loss function\n",
    "optimizer = torch.optim.SGD(multiclass_logit.parameters(), lr = 0.1, weight_decay = 0.01) # SGD optimizer\n",
    "num_epochs = 25\n",
    "\n",
    "validation_acc_list = np.zeros(num_epochs)\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for batch in train_loader:\n",
    "        model_output = multiclass_logit(Variable(torch.squeeze(batch[0], 1).view(len(batch[0]), 28*28))) # model predictions\n",
    "        targets = Variable(batch[1]) # true digit values\n",
    "        \n",
    "        optimizer.zero_grad() # zero gradient\n",
    "        loss_batch = loss_function(model_output, targets) # compute loss\n",
    "        loss_batch.backward() # take the gradient wrt parameters\n",
    "        optimizer.step() # update parameters\n",
    "                \n",
    "    validation_acc_list[epoch] = compute_accuracy(validation_loader, multiclass_logit) # compute validation acc and store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOMMMXBfdKQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2932f1687b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(num_epochs), validation_acc_list, 'bo')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('Validation Accuracy vs. Epoch for Lambda = 0.01')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "lambdas = [0.1, 0.01, 0.001, 0.0001, 0] # values of lambda to try\n",
    "softmax = torch.nn.Softmax() # softmax to compute output probabilities \n",
    "loss_function = torch.nn.CrossEntropyLoss() # cross entropy loss function\n",
    "num_epochs = 40\n",
    "\n",
    "validation_acc_lists = []\n",
    "for lam in lambdas:\n",
    "\n",
    "    multiclass_logit = torch.nn.Linear(in_features = 28*28, out_features = 10, bias = True) # linear layer\n",
    "    optimizer = torch.optim.SGD(multiclass_logit.parameters(), lr = 0.1, weight_decay = lam) # SGD optimizer\n",
    "    \n",
    "    validation_acc_list = np.zeros(num_epochs)\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        for batch in train_loader:\n",
    "            model_output = multiclass_logit(Variable(torch.squeeze(batch[0], 1).view(len(batch[0]), 28*28))) # model predictions\n",
    "            targets = Variable(batch[1]) # true digit values\n",
    "\n",
    "            optimizer.zero_grad() # zero gradient\n",
    "            loss_batch = loss_function(model_output, targets) # compute loss\n",
    "            loss_batch.backward() # take the gradient wrt parameters\n",
    "            optimizer.step() # update parameters\n",
    "\n",
    "        validation_acc_list[epoch] = compute_accuracy(validation_loader, multiclass_logit) # compute validation acc and store\n",
    "        \n",
    "    validation_acc_lists.append(validation_acc_list) # add to lists of validation accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmsAAAJcCAYAAACmM+PxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqcAMa5IkSQX2/wHsqqLc93uwrgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2932cc526a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(len(lambdas)):\n",
    "    plt.plot(np.arange(num_epochs), validation_acc_lists[i], 'o')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('Validation Accuracy vs. Epoch for Different Values of Lambda')\n",
    "plt.legend(loc = 'center right', labels = ['Lambda = 0.1', 'Lambda = 0.01',\\\n",
    "                                   'Lambda = 0.001', 'Lambda = 0.0001',\\\n",
    "                                   'Lambda = 0'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $\\lambda=0.1$, it is sufficient to stop after about 1 epoch. For $\\lambda=0.01$, it is sufficient to stop after about 10 epochs. For $\\lambda=0.001$, it is sufficient to stop after about 35 epochs. For $\\lambda=0.0001$, it is sufficient to stop after about 35 epochs. For $\\lambda=0$, it is sufficient to stop after about 35 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\lambda=0$ appears to be the best regularization parameter, so that the hyperparameter I used when computing test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy when lambda = 0: 0.9224\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy when lambda = 0: ' + str(compute_accuracy(test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Multi-Layer Perceptron\n",
    "\n",
    "The multilayer perceptron can be understood as a logistic regression classifier in which the input is first transformed using a learnt non-linear transformation. The non-linear transformation is usually chosen to be either the logistic function or the $\\tanh$ function or the RELU function, and its purpose is to project the data into a space where it becomes linearly separable The output of this so-called hidden layer is then passed to the logistic regression graph that we have constructed in the first problem. \n",
    "\n",
    "![](http://deeplearning.net/tutorial/_images/mlp.png)\n",
    "\n",
    "We'll construct a model with **1 hidden layer**. That is, you will have an input layer with a nonlinearity, then a hidden layer with the nonlinearity, and finally a cross-entropy (or equivalently log-softmax with a log-loss)\n",
    "\n",
    "\n",
    "Using a similar architecture as in the first part and the same training, validation, and test sets, build a PyTorch model for the multilayer perceptron. Use the $\\tanh$ function as the non-linear activation function. \n",
    "\n",
    "1. Use $\\lambda = 0.001$ to compare with Problem 2. Experiment with the learning rate (try 0.1 and 0.01 for example), batch size (use 20, 50, 100 and 200) and the number of units in your hidden layer (use between 25 and 100 units). For what combination of these parameters do you obtain the highest validation accuracy after a reasonable number of epochs that lead to convergence ( start at 10 epochs and play around a bit for convergence)? How does your test accuracy compare to the logistic regression classifier?\n",
    "2. Try the same values of $\\lambda$ you used in Question 2. Does the test set accuracy improve?\n",
    "\n",
    "\n",
    "*Hint #1:* The initialization of the weights matrix for the hidden layer must assure that the units (neurons) of the perceptron operate in a regime where information gets propagated. For the $\\tanh$ function, you may find it advisable to initialize with the interval $[-\\sqrt{\\frac{6}{fan_{in}+fan_{out}}},\\sqrt{\\frac{6}{fan_{in}+fan_{out}}}]$, where $fan_{in}$ is the number of units in the $(i-1)$-th layer, and $fan_{out}$ is the number of units in the i-th layer.\n",
    "\n",
    "*Hint #2*\n",
    "Train/Validate/Test split can be done in numpy or in PyTorch. Lab will describe a way to do it keeping within the MNIST `DataLoader` workflow: the key is to pass a `SubsetRandomSampler` to `DataLoader`: see the docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters to try\n",
    "lam = 0.001\n",
    "lrs = [0.1, 0.01]\n",
    "batch_size = [20, 50, 100, 200]\n",
    "hidden_units = [50, 75, 100]\n",
    "num_epochs = 35\n",
    "\n",
    "softmax = torch.nn.Softmax() # softmax to compute output probabilities \n",
    "loss_function = torch.nn.CrossEntropyLoss() # cross entropy loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for lambda=0.001, learning rate=0.1, batch size=20, hidden units=50: 0.9188\n",
      "Validation accuracy for lambda=0.001, learning rate=0.1, batch size=20, hidden units=75: 0.921\n",
      "Validation accuracy for lambda=0.001, learning rate=0.1, batch size=20, hidden units=100: 0.9196\n",
      "Validation accuracy for lambda=0.001, learning rate=0.1, batch size=50, hidden units=50: 0.923\n",
      "Validation accuracy for lambda=0.001, learning rate=0.1, batch size=50, hidden units=75: 0.9239\n",
      "Validation accuracy for lambda=0.001, learning rate=0.1, batch size=50, hidden units=100: 0.9223\n",
      "Validation accuracy for lambda=0.001, learning rate=0.1, batch size=100, hidden units=50: 0.9229\n",
      "Validation accuracy for lambda=0.001, learning rate=0.1, batch size=100, hidden units=75: 0.9235\n",
      "Validation accuracy for lambda=0.001, learning rate=0.1, batch size=100, hidden units=100: 0.9239\n",
      "Validation accuracy for lambda=0.001, learning rate=0.1, batch size=200, hidden units=50: 0.9226\n",
      "Validation accuracy for lambda=0.001, learning rate=0.1, batch size=200, hidden units=75: 0.9222\n",
      "Validation accuracy for lambda=0.001, learning rate=0.1, batch size=200, hidden units=100: 0.9228\n",
      "Validation accuracy for lambda=0.001, learning rate=0.01, batch size=20, hidden units=50: 0.9233\n",
      "Validation accuracy for lambda=0.001, learning rate=0.01, batch size=20, hidden units=75: 0.9225\n",
      "Validation accuracy for lambda=0.001, learning rate=0.01, batch size=20, hidden units=100: 0.9225\n",
      "Validation accuracy for lambda=0.001, learning rate=0.01, batch size=50, hidden units=50: 0.9195\n",
      "Validation accuracy for lambda=0.001, learning rate=0.01, batch size=50, hidden units=75: 0.9184\n",
      "Validation accuracy for lambda=0.001, learning rate=0.01, batch size=50, hidden units=100: 0.9193\n",
      "Validation accuracy for lambda=0.001, learning rate=0.01, batch size=100, hidden units=50: 0.9167\n",
      "Validation accuracy for lambda=0.001, learning rate=0.01, batch size=100, hidden units=75: 0.9166\n",
      "Validation accuracy for lambda=0.001, learning rate=0.01, batch size=100, hidden units=100: 0.9169\n",
      "Validation accuracy for lambda=0.001, learning rate=0.01, batch size=200, hidden units=50: 0.9098\n",
      "Validation accuracy for lambda=0.001, learning rate=0.01, batch size=200, hidden units=75: 0.9098\n",
      "Validation accuracy for lambda=0.001, learning rate=0.01, batch size=200, hidden units=100: 0.9107\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "for lr in lrs:\n",
    "    for size in batch_size:\n",
    "        for hidden in hidden_units:\n",
    "            \n",
    "            # create train batches\n",
    "            train_loader = torch.utils.data.DataLoader(dataset = train, batch_size = size, shuffle = True)\n",
    "                \n",
    "            # model architecture\n",
    "            multi_perceptron = torch.nn.Linear(in_features = 28*28, out_features = hidden, bias = True) # linear layer\n",
    "            multi_perceptron.add_module(name = 'tanh', module = torch.nn.Tanh()) # tanh function for hidden layer\n",
    "            multi_perceptron.add_module(name = '2nd linear', module = torch.nn.Linear(in_features = hidden, out_features = 10, bias = True)) # 2nd linear layer\n",
    "            optimizer = torch.optim.SGD(multi_perceptron.parameters(), lr = lr, weight_decay = lam) # SGD optimizer\n",
    "                \n",
    "            for epoch in range(num_epochs):   \n",
    "                for batch in train_loader:\n",
    "                    model_output = multi_perceptron(Variable(torch.squeeze(batch[0], 1).view(len(batch[0]), 28*28))) # model predictions\n",
    "                    targets = Variable(batch[1]) # true digit values\n",
    "\n",
    "                    optimizer.zero_grad() # zero gradient\n",
    "                    loss_batch = loss_function(model_output, targets) # compute loss\n",
    "                    loss_batch.backward() # take the gradient wrt parameters\n",
    "                    optimizer.step() # update parameters\n",
    "                    \n",
    "            print('Validation accuracy for lambda=' + str(lam) + ', learning rate=' + str(lr) + ', batch size='\\\n",
    "                  + str(size) + ', hidden units=' + str(hidden) + ': ' + str(compute_accuracy(validation_loader, multi_perceptron))) # compute validation acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I obtained the highest validation accuracy (0.9239) with $\\lambda=0.001$, learning rate of 0.1, batch size of 100, and 100 hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for lambda=0.001, learning rate=0.1, batch size=100, hidden units=100: 0.9217\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "# best hyperparams\n",
    "lam = 0.001\n",
    "lr = 0.1\n",
    "size = 100\n",
    "hidden = 100\n",
    "\n",
    "# create train batches\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train, batch_size = size, shuffle = True)\n",
    "\n",
    "# model architecture\n",
    "multi_perceptron = torch.nn.Linear(in_features = 28*28, out_features = hidden, bias = True) # linear layer\n",
    "multi_perceptron.add_module(name = 'tanh', module = torch.nn.Tanh()) # tanh function for hidden layer\n",
    "multi_perceptron.add_module(name = '2nd linear', module = torch.nn.Linear(in_features = hidden, out_features = 10, bias = True)) # 2nd linear layer\n",
    "optimizer = torch.optim.SGD(multi_perceptron.parameters(), lr = lr, weight_decay = lam) # SGD optimizer\n",
    "                \n",
    "for epoch in range(num_epochs):   \n",
    "    for batch in train_loader:\n",
    "        model_output = multi_perceptron(Variable(torch.squeeze(batch[0], 1).view(len(batch[0]), 28*28))) # model predictions\n",
    "        targets = Variable(batch[1]) # true digit values\n",
    "\n",
    "        optimizer.zero_grad() # zero gradient\n",
    "        loss_batch = loss_function(model_output, targets) # compute loss\n",
    "        loss_batch.backward() # take the gradient wrt parameters\n",
    "        optimizer.step() # update parameters\n",
    "                    \n",
    "print('Test accuracy for lambda=' + str(lam) + ', learning rate=' + str(lr) + ', batch size='\\\n",
    "        + str(size) + ', hidden units=' + str(hidden) + ': ' + str(compute_accuracy(test_loader, multi_perceptron))) # compute validation acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test accuracy we see here is slightly lower than the test set accuracy from problem 2 when using $\\lambda=0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the best learning rate, number of hidden units, batch size combination from part 1 of problem 3, I computed the validation accuracies when using the values of $\\lambda$ from part 2 of problem 2. For the combination with the highest validation accuracy, I computed the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for lambda=0.1, learning rate=0.1, batch size=100, hidden units=100: 0.8715\n",
      "Validation accuracy for lambda=0.01, learning rate=0.1, batch size=100, hidden units=100: 0.9058\n",
      "Validation accuracy for lambda=0.001, learning rate=0.1, batch size=100, hidden units=100: 0.9235\n",
      "Validation accuracy for lambda=0.0001, learning rate=0.1, batch size=100, hidden units=100: 0.9274\n",
      "Validation accuracy for lambda=0, learning rate=0.1, batch size=100, hidden units=100: 0.9284\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "# best hyperparams from part 1 of problem 3\n",
    "lr = 0.1\n",
    "size = 100\n",
    "hidden = 100\n",
    "\n",
    "softmax = torch.nn.Softmax() # softmax to compute output probabilities \n",
    "loss_function = torch.nn.CrossEntropyLoss() # cross entropy loss function\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train, batch_size = size, shuffle = True) # create train batches\n",
    "\n",
    "num_epochs = 35\n",
    "lambdas = [0.1, 0.01, 0.001, 0.0001, 0] # values of lambda to try\n",
    "for lam in lambdas:\n",
    "    \n",
    "    # model architecture\n",
    "    multi_perceptron = torch.nn.Linear(in_features = 28*28, out_features = hidden, bias = True) # linear layer\n",
    "    multi_perceptron.add_module(name = 'tanh', module = torch.nn.Tanh()) # tanh function for hidden layer\n",
    "    multi_perceptron.add_module(name = '2nd linear', module = torch.nn.Linear(in_features = hidden, out_features = 10, bias = True)) # 2nd linear layer\n",
    "    optimizer = torch.optim.SGD(multi_perceptron.parameters(), lr = lr, weight_decay = lam) # SGD optimizer\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        for batch in train_loader:\n",
    "            model_output = multi_perceptron(Variable(torch.squeeze(batch[0], 1).view(len(batch[0]), 28*28))) # model predictions\n",
    "            targets = Variable(batch[1]) # true digit values\n",
    "\n",
    "            optimizer.zero_grad() # zero gradient\n",
    "            loss_batch = loss_function(model_output, targets) # compute loss\n",
    "            loss_batch.backward() # take the gradient wrt parameters\n",
    "            optimizer.step() # update parameters\n",
    "\n",
    "    print('Validation accuracy for lambda=' + str(lam) + ', learning rate=' + str(lr) + ', batch size='\\\n",
    "          + str(size) + ', hidden units=' + str(hidden) + ': ' + str(compute_accuracy(validation_loader, multi_perceptron))) # compute validation acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I obtained the highest validation accuracy (0.9284) when $\\lambda=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for lambda=0, learning rate=0.1, batch size=100, hidden units=100: 0.9233\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy for lambda=' + str(lam) + ', learning rate=' + str(lr) + ', batch size='\\\n",
    "      + str(size) + ', hidden units=' + str(hidden) + ': ' + str(compute_accuracy(test_loader, multi_perceptron))) # compute validation acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set accuracy (0.9233) improved when I set $\\lambda=0$ and is slightly higher than the test accuracy obtained in problem 2 (0.9224)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.5.3",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "nteract": {
   "version": "0.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
